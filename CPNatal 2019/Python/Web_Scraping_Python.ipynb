{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Web Scraping with Python\n",
    "## Event: Campus Party Natal 2019\n",
    "## Speaker: Breno Santana Santos\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Getting data with Web Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The steps needed to get and extract data from websites are:\n",
    "1. Determine your extraction's goal;\n",
    "2. Check whether your extraction is legal;\n",
    "3. Determine the data source or the target website;\n",
    "4. Get the HTML source of target page;\n",
    "5. Choose the HTML elements that will be extracted;\n",
    "6. Extract the data with your preffered tool of Web Scraping;\n",
    "7. Store the data, if it is necessary.\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Required Background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The required background to perform the activities of Web Scraping are:\n",
    "* Understand the structure (hierarchy) and elements of HTML (HyperText Markup Language);\n",
    "* Understand the syntax of XPath and/or CSS Selector;\n",
    "    * For select/extract the data contained in HTML elements.\n",
    "* Know how to use the needed tools (in this case, Python tools for Web Scraping).\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. HTML hierarchy and elements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/tags_html.jpg\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example of HTML hierarchy:\n",
    "<img src=\"img/structure_html.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition, there are two attributes mostly important: class and id.\n",
    " * class: determine the CSS class of HTML element.\n",
    " * id: unique identificator of HTML element."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you don't have the deep understanding of HTML, I suggest you take the [W3Schools' HTML tutorial](https://www.w3schools.com/html/).\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Quick Review of XPath and CSS Selector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both resources are used to navigate through elements and attributes in HTML pages. In particular, the CSS Locator enables to select the elements based their CSS styles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. XPath"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic syntax to select the nodes/elements in HTML document is:\n",
    "\n",
    "| Expression | Description |\n",
    "|------------|-------------|\n",
    "| nodename | selects all nodes with the name \"nodename\". |\n",
    "| / | selects from the root node. |\n",
    "| // | selects all nodes no matter where they are in the document. |\n",
    "| . | selects the current node. |\n",
    "| .. | selects the parent of the current node. |\n",
    "| @ | selects attributes. |\n",
    "| parent/nodename[n] | selects the n-th \"nodename\" element that is the child of the \"parent\" element. The value of \"n\" starts from one. |\n",
    "| //nodename[@attr_name] | selects all the \"nodename\" elements that have an attribute named \"attr_name\". |\n",
    "| //nodename[@attr_name='attr_value'] | selects all the \"nodename\" elements that have a \"attr_name\" attribute with a value of \"attr_value\". |\n",
    "| * | matches any element node. |\n",
    "| @* | matches any attribute node.|\n",
    "| nodename/@attribute | selects the value of the \"attr_name\" attribute for \"nodename\" element. |\n",
    "| nodename/text() | extracts the textual information of the \"nodename\" element. |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more information about XPath, I suggest you take [W3Schools' XPath tutorial](https://www.w3schools.com/xml/xpath_intro.asp)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. CSS Selector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic syntax to select the nodes/elements in HTML document is:\n",
    "\n",
    "| Selector | Description |\n",
    "|----------|-------------|\n",
    "| .class_node | selects all elements with class=\"class_node\". |\n",
    "|.class1.class2 | selects all elements with both \"class1\" and \"class2\" set within its class attribute. |\n",
    "| #id_node | selects the element with id=\"id_node\". |\n",
    "| * | selects all elements. |\n",
    "| tagname | selects all \"tagname\" elements. |\n",
    "| tagname1,tagname2 | selects all \"tagname1\" elements and all \"tagname2\" elements. |\n",
    "| tagname1 tagname2 | selects all \"tagname2\" elements inside \"tagname1\" elements. |\n",
    "| tagname1 > tagname2 | selects all \"tagname2\" elements where the parent is a \"tagname1\" element. |\n",
    "| [attr_name] | selects all elements with a \"attr_name\" attribute. |\n",
    "| [attr_name=attr_value] | selects all elements with attr_name=\"attr_value\". |\n",
    "| [attr_name~=attr_value] | selects all elements with a \"attr_name\" attribute containing the \"attr_value\" value. |\n",
    "| [attr_name*=attr_value] | selects every element whose \"attr_name\" attribute value contains the substring \"attr_value\". |\n",
    "| nodename:nth-of-type(n) | selects every \"nodename\" element that is the n-th element of its parent. The value of \"n\" starts from one. |\n",
    "| nodename::attr(attr_name) | selects the value of the \"attr_name\" attribute for \"nodename\" element. |\n",
    "| nodename::text | extracts the textual information of the \"nodename\" element. |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more information about CSS Selector, see [W3Schools' CSS Selector Reference](https://www.w3schools.com/cssref/css_selectors.asp)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3. Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| XPath | CSS Selector | Explanation |\n",
    "|-------|--------------|-------------|\n",
    "| /html/body/div | html > body > div | selects all \"div\" elements that are children of \"body\" tag. |\n",
    "| //table | table | selects all \"table\" elements of HTML page. |\n",
    "| /html/body/div[2]//table | html > body > div:nth-of-type(2) table | selects all \"table\" elements contained in the second \"div\" element of \"body\" tag. |\n",
    "| /html/body/* | html > body > * | selects all children of \"body\" tag. |\n",
    "| //p[@class=\"class-1\"] | p.class-1 | selects all \"p\" elements whose \"class\" attribute is equal to the \"class-1\" class. |\n",
    "| //*[@id=\"uid\"] | *#uid | selects all elements, including their children, whose \"id\" attribute is equal to the \"uid\" value. |\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualizing/Inspecting the HTML pages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An important resource for support the activities of Web Scraping is the inspection of HTML pages' elements. This resource allows you to visualize the elements and their attributes, thus, facilitating the process of extraction data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/inspect.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Getting the HTML pages' source"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way of getting the source of HTML pages is by the use of Python __requests__ module. The __get__ method performs an HTTP request and the __content__ attribute represents the HTML page's content. In addition, you should decode the content for the UTF-8 encoding.\n",
    "\n",
    "Other form of getting the source-code is by the Python __urllib__ library's __request__ module. The __urlopen__ returns the HTTPResponse object, which represents a response of an HTTP request. For obtaining the received content by the request, you should use the __read__ method that represents the set of bytes. Similar to previous way, you should decode the content for the UTF-8 encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determining the URL of target page\n",
    "url = \"https://campuse.ro/events/campus-party-natal-2019\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the requests module and the request module of urllib library\n",
    "import requests\n",
    "from urllib import request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the function that gets source of HTML page\n",
    "def getPage(url, is_urlopen=False):\n",
    "    \"\"\"This function gets the HTML page's source in the UTF-8 encoding\"\"\"\n",
    "    html = request.urlopen(url).read() if is_urlopen else requests.get(url).content  # getting the HTML source\n",
    "    html = html.decode(\"utf-8\")  # setting the encode of the HTML page\n",
    "    return html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the page using the requests module\n",
    "html = getPage(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# printing the 200 first characters\n",
    "html[:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the page using the urlopen method\n",
    "html = getPage(url, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# printing the 200 first characters\n",
    "html[:200]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Extracting data with scrapy Selector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The __Selector__ object is a __scrapy__ library's object that __selects and extracts data__ using XPath or CSS Selector notation. In its constructor method, it is necessary to pass the HTML source to the __text__ attribute. The Selector object has two methods to select HTML elements: __xpath__ and __css__. Both methods return __Selector__ or __SelectorList__ objects. We can use these objects to create new Selector ones of specic pieces of the HTML code. The __extract__ method allows to extract a list of data. While the __extract_first__ extracts the first item of returned list by Selector object.\n",
    "\n",
    "In order to get the absolute URLs from the relative ones, you can use the __urllib__ library's __parse__ module. The __urljoin__ method performs this task, i.e., it gets the absolute URLs by the parent and relative URLs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For installing the scrapy library, uncomment the below line.\n",
    "#%pip install scrapy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the scrapy Selector and the urljoin method\n",
    "from scrapy import Selector\n",
    "from urllib.parse import urljoin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the function that gets the links contained in a page\n",
    "def getLinks(url, locator_links, is_css=True, locator_pagination=None):\n",
    "    \"\"\"This method gets the page's links based on a XPath/CSS locator.\"\"\"\n",
    "\n",
    "    # getting the absolute URLs contained in an HTML page, using XPath or CSS Selector\n",
    "    html = getPage(url)\n",
    "    sel = Selector(text=html)\n",
    "    links = sel.css(locator_links).extract() if is_css else sel.xpath(locator_links).extract()  # extracting the relative links\n",
    "    links = [urljoin(url, item) for item in links]  # generating the absolute URLs\n",
    "\n",
    "    # getting the pagination of absolute URLs\n",
    "    if locator_pagination:\n",
    "        for link in list(links):\n",
    "            links_pagination = getLinks(link, locator_pagination)\n",
    "            if links_pagination and not set(links_pagination).issubset(set(links)):\n",
    "                links.remove(link)\n",
    "                links += links_pagination\n",
    "\n",
    "    return links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1. Getting the absolute URLs of CPNatal 2019's event categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the absolute URLs of the CPNatal 2019's event categories (Workshops and Conferences)\n",
    "xpath = \"//div[@class='small-12 medium-6 large-3 columns pink-bg']/a/@href\"\n",
    "links = getLinks(url, xpath, False)\n",
    "print(links)\n",
    "\n",
    "css = \"div[class='small-12 medium-6 large-3 columns pink-bg'] > a::attr(href)\"\n",
    "links = getLinks(url, css)\n",
    "print(links)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2. Getting the absolute URLs with pagination for CPNatal 2019's event categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the absolute URLs with pagination for CPNatal 2019's event categories (Workshops and Conferences)\n",
    "css_pagination = \"span.step-links > span.list > a::attr(href)\"\n",
    "links = getLinks(url, css, locator_pagination=css_pagination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# printing the result\n",
    "print(links)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3. Getting the subevents' URL for each event category of CPNatal 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the absolute URLs of CPNatal 2019's Workshops and Conferences\n",
    "css_categories = \"div.header > a::attr(href)\"\n",
    "links_subevents = [link_subevent for link_cat in links for link_subevent in getLinks(link_cat, css_categories)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# printing the total of URLs\n",
    "print(\"Total of links:\", len(links_subevents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.4. Extracting data of the CPNatal 2019's subevents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the function that extracts the data based on a list of CSS Selector\n",
    "def getData(url, css_list):\n",
    "    \"\"\"This function gets the content and returns it in the list of data. Also, it performs the text cleaning.\"\"\"\n",
    "    html = getPage(url)\n",
    "    sel = Selector(text=html)\n",
    "    return [sel.css(item_css).extract_first().replace(\"\\n\", \"\").replace(\";\", \",\") if sel.css(item_css).extract_first() != None else \"\" for item_css in css_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the list of CSS Selectors to extract data\n",
    "css = [\"div.left > ul.filter-tab-menu > li:nth-of-type(3) > a::text\", # type\n",
    "       \"div.blue-box > span.title::text\", # title\n",
    "       \"div.metadata > span:nth-of-type(1)::text\", # start date\n",
    "       \"div.metadata > span:nth-of-type(2)::text\", # end date\n",
    "       \"div.metadata > span:nth-of-type(3)::text\", # location\n",
    "       \"div.ident-25 > p::text\" # description\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting and extracting the data\n",
    "data = [[\"Type\",\"Title\",\"Start Date\",\"End Date\",\"Location\",\"Description\"]]  # header\n",
    "data.extend([getData(link, css) for link in links_subevents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# printing the five first records and the number of extracted subevents\n",
    "print(data[:5])\n",
    "print(\"Number of extracted subevents:\", len(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.5. Storing the data in a CSV file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to store the data in a CSV file, you can use the __csv__ module. In every file manipulation, it is necessary to open the file to read or write some content. For this operation, you can invoke the __open__ built-in method to open the file for manipulations. This method returns the __TextIOWrapper__ object that is used to __manipulate__ the __selected file__.\n",
    "\n",
    "The __writer__ method of __csv__ module creates a __writer__ object that stores a content within the file. The __writerow__ method inserts a content line within the selected file. While the __writerows__ method inserts a set of content (rows) within the selected file. Both writing methods receive a list as parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the csv module\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the writer object\n",
    "file_writer = csv.writer(open(\"data_cp_natal_2019.csv\", \"w\"), delimiter=\"|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inserting the data within the file\n",
    "file_writer.writerows(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Crawling with scrapy Response objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **Response** objects have the **same** functionalities of **Selector** ones. They can use the **xpath**, **css**, **extract** and **extract_first** methods. In addition, the Response object **keeps track of the URL** where the HTML code was loaded from, i.e., it keeps track of the URL within the response **url** variable/attribute. It **helps us move from one site to another**, so that we can \"crawl\" the web while scraping. From the **follow** method, the Response lets us \"follow\" to a new link.\n",
    "\n",
    "The **Spider** class determines how to perform the **crawl** (follow links) and how to **extract data** from the pages (scraping items). For creating your Spider, you should create a subclass of **scrapy.Spider**. For the Spider class, it is necessary to create the **start_requests** and **parse** methods. The **former** defines the **start point to run** the Spider object. While the **latter** are responsible for **handling the HTML** code. It is needed to create **at least one parse function**. The **scrapy.Request** statement creates a response variable for us. The **url** argument tells us which site to scrape. The **callback** argument tells us where to send the response variable for processing. The **cb_kwargs** argument tells us what parameters are required for the **callback** argument and it receives a dictionary object that represents the parameters and their values for **callback** function. The **follow** method accepts the same arguments of **scrapy.Request**.\n",
    "\n",
    "Finally, the **CrawlerProcess** objects of **scrapy.crawler** module are responsible for defining and executing the Spider object. The **crawl** method receives the **Spider's subclass** that will be used to extract data. The **start** method starts the execution of the defined Spider's subclass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required imports\n",
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determining the URL of target page\n",
    "url = \"https://campuse.ro/events/campus-party-natal-2019\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition of Spider class\n",
    "class SpiderCPNatal(scrapy.Spider):\n",
    "    name = \"cp_natal_2019\"\n",
    "    file_writer = None\n",
    "\n",
    "    # Start point to run the spider\n",
    "    def start_requests(self):\n",
    "        # creating the file and saving the header within the file\n",
    "        self.file_writer = csv.writer(open(\"data_cp_natal_2019_2.csv\", \"w\"), delimiter=\"|\")\n",
    "        self.file_writer.writerow([\"Type\",\"Title\",\"Start Date\",\"End Date\",\"Location\",\"Description\"])\n",
    "\n",
    "        # getting the relative URLs of the CPNatal 2019's event categories (Workshops and Conferences)\n",
    "        args = dict(css = \"div[class='small-12 medium-6 large-3 columns pink-bg'] > a::attr(href)\")\n",
    "        yield scrapy.Request(url = url, callback=self.parse_links, cb_kwargs=args)\n",
    "\n",
    "    def parse_links(self, response, css):\n",
    "        # extracting the relative URLs\n",
    "        links = response.css(css).extract()\n",
    "\n",
    "        # getting the relative URLs with pagination for CPNatal 2019's event categories (Workshops and Conferences)\n",
    "        args = dict(css_pag = \"span.step-links > span.list > a::attr(href)\")\n",
    "        for link in links:\n",
    "            yield response.follow(url = link, callback=self.parse_pagination, cb_kwargs=args)\n",
    "\n",
    "    def parse_pagination(self, response, css_pag):\n",
    "        # extracting the relative URLs with pagination\n",
    "        links = response.css(css_pag).extract()\n",
    "\n",
    "        # getting the relative URLs of CPNatal 2019's Workshops and Conferences\n",
    "        for link in links:\n",
    "            args = dict(css_cat = \"div.header > a::attr(href)\")\n",
    "            yield response.follow(url = link, callback=self.parse_links_subevents, cb_kwargs=args)\n",
    "\n",
    "    def parse_links_subevents(self, response, css_cat):\n",
    "        # extracting the relative URLs of CPNatal 2019's Workshops and Conferences\n",
    "        links = response.css(css_cat).extract()\n",
    "\n",
    "        # getting the data\n",
    "        for link in links:\n",
    "            args = {\"type\": \"div.left > ul.filter-tab-menu > li:nth-of-type(3) > a::text\", # type\n",
    "                    \"title\": \"div.blue-box > span.title::text\", # title\n",
    "                    \"start_date\": \"div.metadata > span:nth-of-type(1)::text\", # start date\n",
    "                    \"end_date\": \"div.metadata > span:nth-of-type(2)::text\", # end date\n",
    "                    \"location\": \"div.metadata > span:nth-of-type(3)::text\", # location\n",
    "                    \"description\": \"div.ident-25 > p::text\" # description\n",
    "            }\n",
    "            yield response.follow(url = link, callback=self.parse_data, cb_kwargs=args)\n",
    "\n",
    "    def parse_data(self, response, **kwargs):\n",
    "        # extracting the data and saving them in the CSV file\n",
    "        row = [response.css(kwargs[item_css]).extract_first().replace(\"\\n\", \"\").replace(\";\", \",\") if response.css(kwargs[item_css]).extract_first() != None else \"\" for item_css in kwargs]\n",
    "        self.file_writer.writerow(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execution Process to run the spider\n",
    "process = CrawlerProcess()  # initiate a CrawlerProcess\n",
    "process.crawl(SpiderCPNatal)  # tell the process which spider to use\n",
    "process.start()  # start the crawling process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Visualizing the extracted data with Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **pandas** library is one of main libraries used to Data Science with Python. It allows you to work with the data in the **tabular format**. For this, there is the **DataFrame** object. The **read_csv** function allows a **DataFrame** object from the **CSV file**. The **info** method permits you to visualize some informations with respect to the data. The **head** method shows the five first records of your DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the pandas library\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the DataFrame object from the extracted data (CSV file)\n",
    "data_df = pd.read_csv(\"data_cp_natal_2019_2.csv\", delimiter=\"|\", header=0, index_col=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualizing some informations with respect to the DataFrame object\n",
    "data_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualizing the five first records\n",
    "data_df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
